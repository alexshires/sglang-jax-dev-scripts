# Multi-Item Scoring Regression Analysis (2026-02-12)

## Executive Summary

We investigated a performance regression in the `aashishrampal/batching` branch where multi-item scoring throughput dropped from a peak of **~141 items/sec** (observed in earlier tests) to **~70 items/sec** (current state).

## Key Findings

### 1. Regression Confirmation
- **Previous Peak (Turn 105)**: 141.22 items/sec (TP=4, Chunk=512).
- **Current Peak (Turn 115)**: 70.16 items/sec (TP=4, Chunk=512).
- **Impact**: ~50% drop in throughput.

### 2. Root Cause Analysis
The performance difference stems from how the two branches handle "multi-item scoring":

- **`aashishrampal/batching` (Fast)**:
    - Supports **internal batching**.
    - When `multi_item_scoring_chunk_size` is set (default 2, later tested with 32/64), it splits the request into multiple smaller sequences (e.g., 500 items -> 16 sequences of 32 items).
    - These sequences are processed in **parallel** as a batch.
    - This keeps the sequence length short (~1000 tokens), staying in the efficient regime of the attention mechanism.

- **`feat/multi-item-scoring` (Slow)**:
    - Forces all items into a **single, packed sequence** (e.g., 1 query + 500 items = ~12,000 tokens).
    - Uses a custom block-diagonal mask to prevent cross-item attention.
    - While theoretically correct, the $O(N^2)$ complexity of the attention mechanism (even with masking) and the overhead of transferring a massive 1GB+ mask to the TPU significantly degrade performance.

### 3. Kernel Instability
- **TP > 1 Crash**: We confirmed that `ragged_paged_attention` with `custom_mask` crashes on `TP > 1` configurations (2x2, 2x4) with a `ValueError: Specified dims ... do not evenly divide`.
- **Workaround**: Running with `TP=1` bypasses the crash but limits the available compute and memory bandwidth.

### 4. Recommendations

1.  **Adopt Batching Strategy**: The "parallel sequences" approach from `aashishrampal/batching` is superior for throughput. We should default to splitting multi-item requests into smaller chunks (e.g., 32-64 items) and processing them as a batch rather than a single packed sequence.
2.  **Fix Kernel**: The `ragged_paged_attention` kernel needs to be patched to support `custom_mask` with Tensor Parallelism (`TP > 1`).
3.  **Optimize Masking**: If packed sequences are required, we must optimize the mask construction (move to device) or use a more efficient sparse attention representation to avoid the 4GB+ transfer overhead.

## Benchmark Data

| Configuration | Branch/Method | Throughput | Notes |
| :--- | :--- | :--- | :--- |
| **2x2 TPU (TP=4)** | `aashishrampal/batching` (Old) | **141.22 items/s** | Internal Batching (Parallel) |
| **2x2 TPU (TP=4)** | `feat/multi-item-scoring` | 70.16 items/s | Single Packed Sequence |
| **2x2 TPU (TP=4)** | `aashishrampal/batching` (New) | ~70 items/s | Regression due to merge? |
| **1x1 TPU (TP=1)** | Reference | ~52 items/s | Baseline |

---
*Report generated by Gemini CLI*
